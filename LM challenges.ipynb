{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##                  Sage: 10 selenium web scraping exercises to do."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape the title and URL of the top 10 posts on the Reddit homepage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(\"http://www.reddit.com\")\n",
    "time.sleep(5)\n",
    "driver.refresh()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems encountered:\n",
    "1. i think there should be tweaks to the initial for loop, where it can skip ads and show the top 10 posts, rn its not dynamic could increase this number later if needed. news articles are also different btw, using try and except statements make me avoid this issue, would have to fix later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an AD/News article, i'll fix it later, maybe :)\n",
      "This is an AD/News article, i'll fix it later, maybe :)\n"
     ]
    }
   ],
   "source": [
    "def reddit():\n",
    "    # inital attempt; this method doesnt get precise information\n",
    "    # for x in range(1, 11):\n",
    "    #     i = str(x)\n",
    "    #     slider.append(driver.find_element(By.XPATH, f'//*[@id=\"AppRouter-main-content\"]/div/div/div[2]/div[2]/div[1]/div[5]/div[{i}]').text.split('\\n'))\n",
    "\n",
    "    # first proposal isnt precise enough, would require too much text formatting\n",
    "    # the 2nd div and last div would be formmated for our for loop\n",
    "\n",
    "    userLink = 'http://www.reddit.com/user/'\n",
    "    user = [] #username and user profile link\n",
    "    postTitle = [] #title and link\n",
    "    subReddit = [] # Subreddit name\n",
    "    postLink = [] # Subreddit link\n",
    "    comments = []  #no. of comments\n",
    "\n",
    "    # post format\n",
    "    likes = [] # no. of upvotes \n",
    "    header = [] # each post has a header which contains the subreddit posted to and user who posted\n",
    "    title = [] # title of the post\n",
    "    info = [] # info in the post, could be video, text, table, etc.\n",
    "    footer = [] # contains total number of comments\n",
    "\n",
    "    for x in range(1,12):\n",
    "        i = str(x)\n",
    "        try:\n",
    "            likes.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[2]').text)\n",
    "            header.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[1]').text)\n",
    "            title.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[2]').text)\n",
    "            info.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[3]').text)\n",
    "            footer.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[4]').text)\n",
    "            postLink.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[2]/div[1]/a').get_attribute('href'))\n",
    "        except NoSuchElementException:\n",
    "            # try:\n",
    "            #     # if post is a news article, the format changes.\n",
    "            #     likes.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[1]').text)\n",
    "            #     header.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[2]/article/div[1]/div[1]').text)\n",
    "            #     title.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[2]/article/div[1]/div[2]').text)\n",
    "            #     # get info if link available\n",
    "            #     info.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[2]/article/div[1]/div[3]').text)\n",
    "            #     footer.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[2]/div').text)\n",
    "            #     postLink.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[2]/article/div[1]/div[2]/div[1]/a').get_attribute('href'))\n",
    "\n",
    "            # except NoSuchElementException:\n",
    "            print(\"This is an AD/News article, i'll fix it later, maybe :)\")\n",
    "            pass\n",
    "\n",
    "    # print(len(header))\n",
    "    # while len(header)<11:\n",
    "    #     i = str(len(header)+1)\n",
    "    #     try:\n",
    "    #         likes.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[2]').text)\n",
    "    #         header.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[1]').text)\n",
    "    #         title.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[2]').text)\n",
    "    #         info.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[3]').text)\n",
    "    #         footer.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[4]').text)\n",
    "    #         postLink.append(driver.find_element(By.XPATH, f'//div[5]/div[{i}]/div/div/div[3]/div[2]/div[1]/a').get_attribute('href'))\n",
    "    #     except NoSuchElementException:\n",
    "    #         print('This is an AD')\n",
    "    #         pass\n",
    "        \n",
    "    for x in range(len(header)-1):\n",
    "        user.append(header[x].split('\\n')[2] + ' : '+ userLink+header[x].split('\\n')[2][2:])\n",
    "        postTitle.append(title[x] + ' : ' + postLink[x])\n",
    "        subReddit.append(header[x].split('\\n')[0])\n",
    "        comments.append(footer[x].split(' ')[0])\n",
    "    \n",
    "    setattr(reddit, 'user', user)\n",
    "    setattr(reddit, 'postTitle' , postTitle) \n",
    "    setattr(reddit, 'comments', comments)  \n",
    "    setattr(reddit, 'likes',likes) \n",
    "\n",
    "\n",
    "reddit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u/killHACKS : http://www.reddit.com/user/killHACKS',\n",
       " 'u/dobbyisafreepup : http://www.reddit.com/user/dobbyisafreepup',\n",
       " 'u/SeXyHuNtEr69420 : http://www.reddit.com/user/SeXyHuNtEr69420',\n",
       " 'u/IvorBrett : http://www.reddit.com/user/IvorBrett',\n",
       " 'u/SpicyThunder335 : http://www.reddit.com/user/SpicyThunder335',\n",
       " 'u/stache914 : http://www.reddit.com/user/stache914',\n",
       " 'u/Ahomelessfish : http://www.reddit.com/user/Ahomelessfish',\n",
       " 'u/Suajj : http://www.reddit.com/user/Suajj']"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reddit is killing third-party applications (and itself). Read more in the comments. : https://www.reddit.com/r/LifeProTips/comments/148wweh/reddit_is_killing_thirdparty_applications_and/',\n",
       " 'Ok Faux News\\nClubhouse : https://www.reddit.com/r/WhitePeopleTwitter/comments/148vp4m/ok_faux_news/',\n",
       " \"What is your secret that you can't tell anyone because it will probably ruin your life? : https://www.reddit.com/r/AskReddit/comments/148b059/what_is_your_secret_that_you_cant_tell_anyone/\",\n",
       " 'Sheep getting vaccinated : https://www.reddit.com/r/thisismylifenow/comments/148ucw7/sheep_getting_vaccinated/',\n",
       " 'Indefinite Blackout: Next Steps, Polling Your Community, and Where We Go From Here : https://www.reddit.com/r/ModCoord/comments/148ks6u/indefinite_blackout_next_steps_polling_your/',\n",
       " 'Well at least he got cake….\\nMichael jacksons lost sextapes : https://www.reddit.com/r/discordVideos/comments/148vobo/well_at_least_he_got_cake/',\n",
       " 'Can you truly forgive a partner after cheating? : https://www.reddit.com/r/ask/comments/148gtxw/can_you_truly_forgive_a_partner_after_cheating/',\n",
       " '??? : https://www.reddit.com/r/Tinder/comments/148xt57/_/']"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.postTitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2', '1.1k', '21.4k', '1.7k', '9.9k', '326', '8.9k', '362']"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['24.3k',\n",
       " 'r/politics\\n•Posted by\\nu/Picture-unrelated\\n9 hours ago\\nAOC says idea Trump is victim of a ‘two-tier’ justice system is an insult to Black and brown Americans\\nindependent.co.uk/news/w...\\nJoin\\n714 Comments\\nShare\\nSave',\n",
       " '33.0k',\n",
       " '23.7k',\n",
       " '37.6k',\n",
       " '15.1k',\n",
       " '12.0k',\n",
       " '6.3k',\n",
       " '5.5k',\n",
       " '4.8k']"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.reddit.com/r/ask/comments/148gtxw/can_you_truly_forgive_a_partner_after_cheating/'"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i=\"3\"\n",
    "# /html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[2]/div[1]/div[5]/div[1]/div/div/div[3]/div[3]/div/div[2]/div\n",
    "# /html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[2]/div[1]/div[5]/div[3]/div/div/div[3]/div[2]/div[1]/a\n",
    "# /html/body/div[1]/div/div[2]/div[2]/div/div/div/div[2]/div[2]/div[1]/div[5]/div[4]/div/div/div[3]/div[3]/a\n",
    "\n",
    "driver.find_element(By.XPATH, '//div[5]/div[4]/div/div/div[3]/div[2]/div[1]/a').get_attribute('href')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18.3k'"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, '//div[5]/div[2]/div/div/div[1]').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Promoted' in driver.find_element(By.XPATH, '//div[5]/div[14]/div/div/div/div[3]/div/div[1]/div').text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrape the current weather conditions (temperature, humidity, wind speed, etc.) for a specific location from a weather website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get('https://www.wunderground.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(city):\n",
    "    return city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "city = 'shenyang'\n",
    "search = driver.find_element(By.XPATH, '//div/div/lib-search/div/div/div/input')\n",
    "search.send_keys(city)\n",
    "time.sleep(3)\n",
    "search.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date, time, currtemp, sun \n",
    "tdyForecast = []\n",
    "# date, time, sun, forecast text and precipitation\n",
    "tnForecast = []\n",
    "# list would contain the celsius equivalent of the currTemp, tonight, tomorrow and tomorrow night\n",
    "Cel = []\n",
    "# current feel of the temperature on the body  \n",
    "feelsLike = ''\n",
    "# tomorrow forecast; would contain date, temp, forecast text and precipitation\n",
    "tmrForecast = []\n",
    "# tomorrow night's forecast; would contain date, temp, forecast text and precipitation\n",
    "tmrNightForecast = []\n",
    "# precipitation, pollen in air, air quality, UV index, pressure, visibility, clouds, humidity\n",
    "others = []\n",
    "# sunrise, sunset, length of day\n",
    "astronomy = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['access_time 6:04 PM CST on June 15, 2023 (GMT +8) | Updated just now',\n",
       " '--° | 67°',\n",
       " '83 °F',\n",
       " 'LIKE 85°',\n",
       " 'Fair',\n",
       " 'N',\n",
       " '9',\n",
       " \"Tomorrow's temperature is forecast to be WARMER than today.\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_element(By.XPATH, '//div/section/div[3]/div[1]/div/div[1]/div[1]').text.split('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scrape the product details (name, price, image, description, etc.) from an eCommerce website."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scrape the lyrics of your favorite songs from a music website.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Scrape the list of trending topics on Twitter and the number of tweets associated with each topic.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Scrape the list of all the restaurants in a particular city from Yelp."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Scrape the list of all the books in a particular category from Amazon."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Scrape the list of all the movies in a particular genre from IMDB."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Scrape the list of all the jobs posted on a job search website."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Scrape the list of all the headlines from a news website."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
